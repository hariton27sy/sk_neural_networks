# Домашнее задание 1
[Задания](https://github.com/imm-rl-lab/UrFU_course/blob/master/Homework/Homework_1.pdf)

## Задание 1
Задание 1 находится в файле homework1_1.py

Оно похоже на то, что мы решали на практике. Но в данном случае действие возвращает сразу одно из 500 состояний в виде числа.

Так как состояний больше чем в задаче на практике, то тут необходимо либо огромное количество сессий, что увеличивает время работы, либо брать меньший квантиль, тогда средняя награда уменьшается, но находятся пути для большего числа состояний

## Задание 2
Код для сглаживания находится в файле homework1_2.py внутри класса Agent.
* Сглаживание по policy - `update_policy_with_smoothing`
* Сглаживание по Лапласу - `update_policy_with_laplace_smoothing`

Небольшой отчет
* Сглаживание по Лапласу работает как сглаживание между действиями в состоянии, сглаживает разность между ними. Таким образом для действий, которых не было в элитных последовательностях, остается некоторая вероятность чтобы их выполнить, чем больше коэффициент тем ближе policy к равновероятной матрице.
* Сглаживание по policy учитывается предыдущую policy, тогда новая policy не забывает прошлую, а с некоторым коэффициентом учитывает ее.